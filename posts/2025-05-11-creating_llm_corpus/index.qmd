---
title: Building an LLM-generated text corpus for statistical analysis
author: "Paweł Wieczyński"
date: "2025-05-11"
categories: [python, large language models, ollama, openai, sql, NLP]
execute:
  eval: false 
format:
  html:
    toc: true
---

## Introduction

Texts in human languages exhibit fascinating statistical patterns. Many of these patterns persist across diverse languages, suggesting they reflect universal cognitive phenomena in human brains. One of the most famous examples is Zipf's law, which states that the frequency of a word is inversely proportional to its rank in the frequency table.

With the increasing popularity of large language models (LLMs), it's worth investigating whether texts generated by these models exhibit similar statistical patterns to human-written texts. Such comparisons could:

- Reveal how well LLMs capture the statistical properties of human languages
- Highlight if LLMs generate text in fundamentally different ways than humans do
- Enable the development of tools for detecting artificially generated content
- Show variations in statistical properties based on model architecture, temperature settings, tokenization methods, and other factors

In this blog post, we'll create a methodology for building a corpus of LLM-generated texts, which will serve as the foundation for future statistical analyses.


```{python}
import sqlite3
import pandas as pd
import ollama
import openai
import os
from dotenv import load_dotenv
```

## Corpus design

To perform meaningful statistical analysis, we need a well-structured corpus of texts generated by different LLMs. Our corpus should:

1. Include texts from multiple LLM providers (Ollama, OpenAI, etc.)
2. Cover diverse topics and writing styles
3. Allow for controlled experiments by varying parameters like temperature
4. Be stored in a way that facilitates easy retrieval and analysis

We'll use SQLite to store our corpus, as it provides a lightweight database solution that doesn't require a separate server.

```{python}
# Connect to SQLite database (will be created if the file doesn't exist)
conn = sqlite3.connect('llm_corpus.db')
cursor = conn.cursor()

# Create table for prompts
cursor.execute('''
CREATE TABLE IF NOT EXISTS prompts (
    id INTEGER PRIMARY KEY,
    prompt TEXT NOT NULL
)
''')

# Create table for generated texts with metadata
cursor.execute('''
CREATE TABLE IF NOT EXISTS texts (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    text TEXT NOT NULL,
    model_name TEXT NOT NULL,
    temperature REAL NOT NULL,
    prompt_id INTEGER NOT NULL,
    system_prompt TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (prompt_id) REFERENCES prompts (id)
)
''')

# Commit the schema changes
conn.commit()
```

## Designing diverse text prompts

A diverse set of prompts will help us generate a representative corpus that captures different aspects of language use. To avoid biasing our corpus with our own preferences, we used a metaprompt approach to generate prompt ideas. We asked [Perplexity AI](https://www.perplexity.ai/) to generate a list of diverse themes for our text corpus. The metaprompt was:

```
I want to create a corpus of texts generated by different large language models. For this purpose I need a set of initial prompts. One example is "Write a long story about a dragon and a princess.". Give me several other examples, so I will have a thematically diversified set of prompts.
```

This meta-prompting approach yielded 15 diverse prompts:

1. Write a short horror story about a haunted house and the people who dare to enter it
2. Imagine you are a travel blogger. Write a post about your visit to a remote island in the Pacific.
3. Discuss the meaning of happiness and whether it is something that can be pursued or if it is a byproduct of other actions.
4. Write about a traditional festival from a culture other than your own, explaining its significance and customs.
5. Describe how artificial intelligence is likely to impact healthcare in the next decade.
6. Provide step-by-step instructions on how to bake a chocolate cake from scratch.
7. Write a conversation between two friends who are debating whether to move to a new city.
8. Argue for or against the statement: "Social media does more harm than good to society."
9. Reflect on a moment in your life when you learned an important lesson. What happened, and how did it change you?
10. Write a poem about the changing of the seasons, focusing on the transition from autumn to winter.
11. Explain the process of photosynthesis in simple terms suitable for a 10-year-old.
12. Create a detailed description of a magical forest and the creatures that inhabit it.
13. Imagine you are a soldier during the American Civil War. Write a letter home describing your experiences.
14. Describe a day in the life of an astronaut living on a space station orbiting Jupiter.
15. Write a long story about a detective solving a mystery in a futuristic city.

These prompts cover various genres (fiction, non-fiction, educational), formats (stories, essays, dialogues, poems), and topics, providing a good foundation for our corpus.

```{python}
# Import prompts from CSV file and store them in the database
def store_prompt(prompt):
    """Store a single prompt in the database and return its ID"""
    cursor.execute('INSERT INTO prompts (prompt) VALUES (?)', (prompt,))
    conn.commit()
    return cursor.lastrowid

prompts_df = pd.read_csv("initial_prompts.csv", sep=";", header=None, names=["id", "prompt"])

for _, row in prompts_df.iterrows():
    prompt = row['prompt']
    store_prompt(prompt)
    
print(f"Stored {len(prompts_df)} prompts in the database.")
```

## Building a text generation framework

To systematically collect texts from different LLMs, we need a unified interface that can work with multiple model providers. We'll create a `TextGenerator` class that abstracts away the differences between model APIs and provides consistent functionality.

Our framework needs to:
1. Support multiple LLM providers (Ollama, OpenAI, etc.)
2. Allow customization of generation parameters (temperature, system prompts)
3. Provide a consistent interface for text generation
4. Handle API-specific requirements and error cases

```{python}
class TextGenerator:
    """
    A class to generate text using various LLM providers with a unified interface.
    
    This class abstracts away the differences between different LLM APIs,
    providing a consistent way to generate text regardless of the underlying model.
    """
    def __init__(self, model_name, temperature=0.7, provider="ollama"):
        """
        Initialize the text generator
        
        Parameters:
        -----------
        model_name : str
            Name of the model to use (e.g., "gemma3:1b", "gpt-3.5-turbo")
        temperature : float
            Temperature parameter for controlling randomness in text generation
            Higher values (e.g., 0.8) produce more diverse outputs
            Lower values (e.g., 0.2) produce more deterministic outputs
        provider : str
            Provider of the model ("ollama" or "openai")
        """
        self.model_name = model_name
        self.temperature = temperature
        self.provider = provider
        
        # Configure provider-specific clients
        if provider == "openai":
            load_dotenv()  # Load API keys from .env file
            self.client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    def generate_text(self, prompt, system_prompt=None):
        """
        Generate text based on the given prompt
        
        Parameters:
        -----------
        prompt : str
            The prompt to generate text from
        system_prompt : str, optional
            System prompt to guide the model's behavior
            
        Returns:
        --------
        str
            Generated text
        """
        if self.provider == "ollama":
            response = ollama.generate(
                model=self.model_name,
                prompt=prompt,
                system=system_prompt if system_prompt else "",
                options={"temperature": self.temperature}
            )
            return response['response']
            
        elif self.provider == "openai":
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=self.temperature
            )
            return response.choices[0].message.content
        
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")
```

```{python}
def generate_and_store_texts(generator, prompts=None, system_prompt=None):
    """
    Generate texts for all prompts using the provided text generator and store them in the database
    
    Parameters:
    -----------
    generator : TextGenerator
        The text generator instance to use
    prompts : list, optional
        List of prompts to use. If None, fetches all prompts from the database.
    system_prompt : str, optional
        System prompt to guide the model's behavior
        
    Returns:
    --------
    int
        Number of texts generated
    """
    # Fetch prompts if not provided
    if prompts is None:
        cursor.execute('SELECT * FROM prompts')
        prompts = cursor.fetchall()
    
    count = 0
    for prompt in prompts:
        prompt_id = prompt[0]
        prompt_text = prompt[1]
        
        # Generate text using the provided generator
        generated_text = generator.generate_text(prompt_text, system_prompt)
        
        # Store the generated text with metadata
        cursor.execute('''
        INSERT INTO texts (text, model_name, temperature, prompt_id, system_prompt)
        VALUES (?, ?, ?, ?, ?)
        ''', (generated_text, generator.model_name, generator.temperature, prompt_id, system_prompt))
        
        # Commit after each text to avoid losing progress if interrupted
        conn.commit()
        
        count += 1
    
    return count
```

## Corpus generation process

Now that we have our database schema and text generation framework in place, we can start building our corpus. We'll generate texts using different models and store them in our database for later analysis.

```{python}
# Retrieve all prompts from the database
cursor.execute('SELECT * FROM prompts')
prompts = cursor.fetchall()
print(f"Retrieved {len(prompts)} prompts from the database.")
```

We will create system prompt to guide behaviour of the LLMs. Without it, for instance every answer from *gemma3* model begins like *"Okay, here's a short story about..."*.

```{python}
system_prompt = "Please ONLY the answer to the prompt. Do not include any other text."
```

### Generating texts with local models

Ollama allows us to run open-source models locally. This is particularly useful for:
- Preserving privacy by keeping generated data local
- Avoiding API costs associated with cloud-based solutions
- Testing smaller or specialized models not available via API services

For this example, we'll use [gemma3 (1B parameter version)]((https://ollama.com/library/gemma3:1b)), a recent lightweight model from Google.

```{bash}
# Start the Ollama model
ollama run gemma3:1b
```

```{python}
ollama_model = TextGenerator(model_name="gemma3:1b", provider="ollama")

for i in range(1):
    generate_and_store_texts(ollama_model, prompts, system_prompt)
```

```{bash}
# Stop the Ollama
ollama stop gemma3:1b
```

### Generating texts via API

Cloud-based models like those from OpenAI often provide state-of-the-art capabilities but require API access. Using these models allows us to:
- Test cutting-edge models with greater capabilities
- Compare commercial models with open-source alternatives
- Examine differences between model families and architectures

For our cloud-based generation, we'll use *GPT-4o mini* from OpenAI.

```{python}
openai_model = TextGenerator(model_name="gpt-4o-mini-2024-07-18", provider="openai")

for i in range(1):
    generate_and_store_texts(openai_model, prompts, system_prompt)
```

## References

This blog post was written with the assistance of the *Claude 3.7 Snonnet Thinking* model.

