[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nBuilding an LLM-generated text corpus for statistical analysis\n\n\n\n\n\n\npython\n\n\nlarge language models\n\n\nollama\n\n\nopenai\n\n\nsql\n\n\nNLP\n\n\n\n\n\n\n\n\n\nMay 11, 2025\n\n\nPaweł Wieczyński\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian Process Regression\n\n\n\n\n\n\ngaussian process\n\n\nregression\n\n\npython\n\n\ntorch\n\n\nBayesian\n\n\n\n\n\n\n\n\n\nMar 6, 2025\n\n\nPaweł Wieczyński\n\n\n\n\n\n\n\n\n\n\n\n\nGrammar-based data compression\n\n\n\n\n\n\ngrammar\n\n\ncompression\n\n\npython\n\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\nPaweł Wieczyński\n\n\n\n\n\n\n\n\n\n\n\n\nWord2Vec skip-gram from scratch in R\n\n\n\n\n\n\nNLP\n\n\ntorch\n\n\nR\n\n\n\n\n\n\n\n\n\nFeb 7, 2025\n\n\nPaweł Wieczyński\n\n\n\n\n\n\n\n\n\n\n\n\nTest document\n\n\n\n\n\n\nblog\n\n\ntest\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\nPaweł Wieczyński\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-02-23-grammar_based_data_compression/index.html",
    "href": "posts/2025-02-23-grammar_based_data_compression/index.html",
    "title": "Grammar-based data compression",
    "section": "",
    "text": "Let \\(\\mathbb{X}\\) denote fixed finite alphabet (in case of character level encoders) or fixed vocabulary (in case of word level encoders). We have a string \\(x \\in \\mathbb{X}^*\\) to be compressed. Based on paper by Kieffer and Young, it can be achieved efficiently in a two-step procedure:\n\ngrammar transform converts string \\(x\\) into grammar \\(G_x\\)\ngrammar encoder converts grammar \\(G_x\\) into binary string \\(B(G_x)\\).\n\nThey define a context-free grammar as a quadruple \\(G = (V, T, P, S)\\) where:\n\n\\(V\\) is a finite nonempty set of non-terminal symbols\n\\(T\\) is a finite nonempty set of terminal symbols\n\\(P\\) is a finite set of production rules \\(V \\to (V \\cup T)^*\\)\n\\(S \\in V\\) is a start symbol."
  },
  {
    "objectID": "posts/2025-02-23-grammar_based_data_compression/index.html#introduction",
    "href": "posts/2025-02-23-grammar_based_data_compression/index.html#introduction",
    "title": "Grammar-based data compression",
    "section": "",
    "text": "Let \\(\\mathbb{X}\\) denote fixed finite alphabet (in case of character level encoders) or fixed vocabulary (in case of word level encoders). We have a string \\(x \\in \\mathbb{X}^*\\) to be compressed. Based on paper by Kieffer and Young, it can be achieved efficiently in a two-step procedure:\n\ngrammar transform converts string \\(x\\) into grammar \\(G_x\\)\ngrammar encoder converts grammar \\(G_x\\) into binary string \\(B(G_x)\\).\n\nThey define a context-free grammar as a quadruple \\(G = (V, T, P, S)\\) where:\n\n\\(V\\) is a finite nonempty set of non-terminal symbols\n\\(T\\) is a finite nonempty set of terminal symbols\n\\(P\\) is a finite set of production rules \\(V \\to (V \\cup T)^*\\)\n\\(S \\in V\\) is a start symbol."
  },
  {
    "objectID": "posts/2025-02-23-grammar_based_data_compression/index.html#character-level-encoder",
    "href": "posts/2025-02-23-grammar_based_data_compression/index.html#character-level-encoder",
    "title": "Grammar-based data compression",
    "section": "Character level encoder",
    "text": "Character level encoder\n\nLempel-Ziv parser\nFirst, we need to compress original string \\(x \\in \\mathbb{X}^*\\) into a list of substrings. This can be done using Lempel-Ziv algorithm. There are two variations of this algorithm: LZ77 (sliding window) and LZ78 (tree-structure). The latter one parses a string into substrings, where each substring is the shortest substring not seen before. More detail on Lempel-Ziv coding can be found in Chapter 13.4 of the textbook by Cover and Thomas. Below class LempelZivParser is an implementation of LZ78.\n\nclass LempelZivParser:\n  # Implementation of LZ78 algorithm\n  def __init__(self, string: str):\n    self.string: str = string\n    self.substrings: list[str] = []\n    \n  def parse(self) -&gt; None:\n    n: int = len(self.string)\n    i: int = 0\n    while i &lt; n:\n      j: int = 1\n      while i + j &lt;= n and self.string[i:(i+j)] in self.substrings:\n        j += 1\n      self.substrings.append(self.string[i:(i+j)])\n      i += j\n\nExample string \\(s = \\text{abaabaaaaaab}\\) appears throughout the paper of Kieffer and Young.\n\ns = \"abaabaaaaaab\"\nlz = LempelZivParser(s)\nlz.parse()\nprint(lz.substrings)\n\n['a', 'b', 'aa', 'ba', 'aaa', 'aab']\n\n\n\n\nGrammar transform\nLet \\(G_x\\) be a context-free grammar of a string \\(x\\in \\mathbb{X}^*\\) parsed by the LZ78 into a list of substrings \\(\\lbrace s_1, s_2, \\dots , s_n \\rbrace\\). We will store \\(G_x\\) as a dictionary where keys are non-terminal symbols from \\(V\\) and values are strings from \\((V \\cup T)^*\\). The algorithm to create \\(G_x\\) is as follows:\n\nfor each substring \\(s_i\\), \\(i=1,2, \\dots n\\):\n\nif \\(|s_i| = 1\\) and \\(s_i \\in T\\), then create a production rule \\(A_i \\to s_i\\)\nif \\(|s_i| &gt; 1\\), then\n\nsplit substring \\(s_i\\) into \\(s_{\\operatorname{prefix}} = s_i [1:(|s_i| - 1)]\\), \\(s_{\\operatorname{suffix}} = s_i[|s_i|]\\)\nby construction of LZ78, there already exists a production rule \\(A_j\\) for \\(s_{\\operatorname{prefix}}\\), so we create a production rule \\(A_i \\to A_j \\ s_{\\operatorname{suffix}}\\)\n\n\nThe start symbol \\(A_0\\) expands to the concatenation of all non-terminal symbols in the order they were introduced, i.e. \\(A_0 \\to A_1 \\ A_2 \\ \\cdots \\ A_n\\).\n\n\n\nGrammar encoder\nWe encode grammar \\(G_x\\) into a binary string using the following algorithm:\n\ncount of production rules, \\(|P|\\), encoded with unary coding, i.e. \\(n \\in \\mathbb{N}_0\\) represented by \\(n\\) \\(1\\)’s followed by single \\(0\\)\nfor each production rule \\(p \\in P\\):\n\nindex of non-terminal symbol encoded with unary coding\nfor each symbol \\(s\\) in the right-hand side of the production rule \\(p\\):\n\nindicator bit whether \\(s\\) is terminal (\\(0\\)) or non-terminal (\\(1\\))\nif \\(s\\) is non-terminal, encode its index with unary coding\nif \\(s\\) is terminal, append its 8-bit ASCII code.\n\nrule terminator \\(10\\) indicating end of production rule \\(p\\).\n\n\nFrom the above, the binary representation of each production rule has the following form:\n\\[\n\\underbrace{(1\\cdots1\\,0)}_{\\text{non-terminal index in unary}}\n\\quad\n\\prod_{s \\in \\mathrm{RHS}(p)} \\Bigl[\n  \\underbrace{0}_{\\text{terminal indicator}}\n  + \\underbrace{\\text{(8-bit ASCII of }s)}_{\\text{if terminal}}\n  \\quad\\text{or}\\quad\n  \\underbrace{1}_{\\text{non-terminal indicator}}\n  + \\underbrace{(1\\cdots1\\,0)}_{\\text{non-terminal index in unary}}\n\\Bigr]\n\\quad\n10.\n\\]\n\nclass GrammarEncoder:\n  \"\"\"\n  A class for building and encoding a grammar from a list of substrings. Each substring is transformed into production rules. References to repeated substrings are replaced with non-terminal symbols. Finally, the grammar can be encoded into a prefix-free binary string.\n  \"\"\"\n  def __init__(self, substrings: list[str]):\n    self.substrings: list[str] = substrings\n\n    # A dictionary to store the grammar rules, mapping a non-terminal symbol (e.g., \"A1\") to a list of right-hand side symbols (terminals or non-terminals).\n    self.grammar: dict[str, list[str]] = {}\n\n    # A string that will hold the final binary encoding of the grammar.\n    self.binary: str = \"\"\n\n    # A dictionary used to keep track of prefix-to-non-terminal mappings. For example, if \"ab\" has been encoded as \"A2\", then prefixes[\"ab\"] = \"A2\".\n    # TBD: Can it be done without separate dictionary?\n    self.prefixes: dict[str, str] = {}\n     \n  def build_grammar(self) -&gt; None:\n    \"\"\"\n    Build the grammar rules based on the provided substrings.\n\n    For each substring:\n        - If the substring is a single character, it is treated as a terminal and assigned a new non-terminal symbol, e.g. A1 -&gt; 'a'.\n        - If the substring has multiple characters, split it into the prefix (all but the last character) and the last character. The prefix itself should already be assigned a non-terminal symbol; the new rule thus references that non-terminal and the final character, e.g. A2 -&gt; A1 'b'.\n\n    Finally, create a start symbol A0 that references all created non-terminals, and move it to the front of the grammar dictionary.\n    \"\"\"\n    # Iterate through each substring, assigning a non-terminal symbol.\n    for i, substring in enumerate(self.substrings, start = 1):\n        if len(substring) == 1:\n            # Single terminal character: create a rule like A1 -&gt; 'a'\n            self.grammar[f\"A{i}\"] = [substring]\n        else:\n            # Multiple-character substring: split into prefix and last char\n            prefix = substring[:-1]\n            last_char = substring[-1]\n            # The prefix must already have an associated non-terminal\n            self.grammar[f\"A{i}\"] = [self.prefixes[prefix], last_char]\n\n        # Store the mapping from the full substring to the new non-terminal\n        self.prefixes[substring] = f\"A{i}\"\n    \n    # The start symbol references all created non-terminals (A1, A2, etc.)\n    n = len(self.substrings)\n    self.grammar[\"A0\"] = [f\"A{i}\" for i in range(1, n + 1)]\n    \n    # Move the start symbol A0 to the beginning of the dictionary.\n    start_symbol = self.grammar.pop(\"A0\")\n    self.grammar = {\"A0\": start_symbol, **self.grammar}\n  \n  def encode(self) -&gt; None:\n    \"\"\"\n    Encode the grammar into a prefix-free binary string.\n    \n    Encoding format:\n        1. Number of rules in unary (e.g., for N rules, '1'*N + '0').\n        2. For each rule of the form A -&gt; α:\n            - Encode A's index in unary (e.g., A3 -&gt; '1110').\n            - For each symbol in α:\n                * Output '0' followed by 8-bit ASCII if it's a terminal.\n                * Output '1' followed by the non-terminal index in unary if it's a non-terminal.\n            - Terminate each rule with '10' (arbitrary separator).\n\n    Example of a single symbol encoding:\n        - Terminal 'a' -&gt; '0' + ASCII('a').\n        - Non-terminal A4 -&gt; '1' + '11110'.\n    \"\"\"\n    # Number of rules in unary code.\n    num_rules = len(self.grammar)\n    self.binary += \"1\" * num_rules + \"0\"\n\n    # Encode each grammar rule (key -&gt; value).\n    for key, value in self.grammar.items():\n      # Encode the non-terminal (e.g., A3 -&gt; '1110').\n      self.binary += \"1\" * int(key[1:]) + \"0\"\n\n      # For each symbol on the right-hand side.\n      for symbol in value:\n        if symbol.startswith(\"A\"):\n            # Non-terminal: '1' + unary representation of the symbol index + '0'.\n            self.binary += \"1\" + (\"1\" * int(symbol[1:])) + \"0\"\n        else:\n            # Terminal: '0' + 8-bit ASCII representation of the character.\n            self.binary += \"0\" + format(ord(symbol), \"08b\")\n\n      # '10' is used as a separator for the end of each rule.\n      self.binary += \"10\"\n\n\nG = GrammarEncoder(lz.substrings)\nG.build_grammar()\nfor rule in G.grammar.items():\n    print(rule)\n\n('A0', ['A1', 'A2', 'A3', 'A4', 'A5', 'A6'])\n('A1', ['a'])\n('A2', ['b'])\n('A3', ['A1', 'a'])\n('A4', ['A2', 'a'])\n('A5', ['A3', 'a'])\n('A6', ['A3', 'b'])\n\n\n\nG.encode()\nprint(G.binary)\n\n1111111001101110111101111101111110111111101010001100001101100011000101011101100011000011011110111000110000110111110111100011000011011111101111000110001010"
  },
  {
    "objectID": "posts/2025-02-23-grammar_based_data_compression/index.html#character-level-decoder",
    "href": "posts/2025-02-23-grammar_based_data_compression/index.html#character-level-decoder",
    "title": "Grammar-based data compression",
    "section": "Character level decoder",
    "text": "Character level decoder\n\nclass GrammarDecoder:\n  \"\"\"\n  A class for decoding a grammar from a given prefix-free binary string (the format produced by GrammarEncoder class). The class provides functionality to parse the binary-encoded grammar, store it in a dictionary, and then expand the grammar rules to reconstruct the original string or phrases.\n  \"\"\"\n  def __init__(self, binary: str):\n    # TBD: add check if its string of 0's and 1's\n    self.binary: str = binary\n\n    # The current reading position in the binary string.\n    self.position: int = 0\n\n    # A dictionary to store the decoded grammar rules, mapping from non-terminal (e.g., \"A3\") to its right-hand side (a list of terminals or non-terminals).\n    self.grammar: dict[str, str] = {}\n\n    # A placeholder for the final decoded string (if needed).\n    self.string: str = \"\"\n  \n  def _parse_unary(self) -&gt; int:\n    \"\"\"\n     Parse a unary number from the current position in the binary data. The unary number is represented by a consecutive run of '1' characters followed by a single '0'. For example, \"1110\" represents the number 3, and \"10\" represents 1.\n    \"\"\"\n    count = 0\n    # Count the consecutive '1's.\n    while self.position &lt; len(self.binary) and self.binary[self.position] == \"1\":\n      count += 1\n      self.position += 1\n    # Consume the '0' which terminates the unary representation.\n    if self.position &lt; len(self.binary) and self.binary[self.position] == \"0\":\n      self.position += 1\n    else:\n      raise ValueError(\"Unary parse error: missing terminating '0'.\")\n    return count\n  \n  def _parse_symbol(self):\n    \"\"\"\n    Parse the next symbol (terminal or non-terminal) from the binary data. Returns a string:\n        - If the symbol is a terminal, the corresponding ASCII character.\n        - If the symbol is a non-terminal, the string \"A\" followed by its index (e.g., \"A3\").\n    \"\"\"\n    # Read the indicator bit: '0' for terminal, '1' for non-terminal.\n    indicator = self.binary[self.position]\n    self.position += 1\n\n    if indicator == \"0\":\n        # Next 8 bits represent the ASCII code of a terminal.\n        if self.position + 8 &gt; len(self.binary):\n          raise ValueError(\"Terminal parse error: not enough bits for ASCII code.\")\n        symbol = self.binary[self.position:(self.position + 8)]\n        self.position += 8\n        return chr(int(symbol, base = 2))\n    elif indicator == \"1\":\n        # Non-terminal =&gt; parse the unary-coded index to get something like \"A3\".\n        non_terminal_index = self._parse_unary()\n        return f\"A{non_terminal_index}\"\n\n  def decode(self) -&gt; None:\n    \"\"\"\n    Decode the grammar from the stored binary data and populate the 'grammar' dictionary.\n\n    The binary format is assumed to be:\n        1. The number of rules, in unary code (e.g., '1110' =&gt; 3 rules).\n        2. For each rule:\n            - A non-terminal index in unary code (e.g., A2 =&gt; '110').\n            - A sequence of symbols (terminals or non-terminals), each with:\n                * '0' + 8 bits for a terminal's ASCII code, or\n                * '1' + unary code for another non-terminal.\n            - A '10' delimiter ending the rule.\n    \"\"\"\n    # Parse the number of rules from the unary code.\n    num_rules = self._parse_unary()\n    if num_rules &lt; 1:\n      raise ValueError(\"Number of rules must be greater that 0.\")\n    \n    # Prepare a container for all production rules.\n    production_rules = [None] * num_rules\n\n    # Parse each rule, reading its non-terminal index and right-hand side.\n    for _ in range(num_rules):\n      # Parse the non-terminal index for the rule (e.g. A3).\n      non_terminal_index = self._parse_unary()\n      rhs_symbols = []\n\n      # Keep parsing symbols until encountering '10', which ends the rule.\n      while True:\n        # Check if the next bits are '10' =&gt; end of rule.\n        if self.position + 2 &lt;= len(self.binary) and self.binary[self.position:(self.position + 2)] == \"10\":\n          self.position += 2\n          break\n        # Otherwise, parse the next symbol.\n        rhs_symbols.append(self._parse_symbol())\n\n      # Ensure the non-terminal index is valid.\n      if non_terminal_index &gt;= num_rules:\n        raise ValueError(f\"Rule index {non_terminal_index} out of range for num_rules={num_rules}.\")\n\n      # Assign the parsed symbols to the corresponding rule index.\n      production_rules[non_terminal_index] = rhs_symbols\n        \n    # Map the rule index to the form \"A{index}\" in the grammar dictionary.\n    for i, rhs in enumerate(production_rules):\n      self.grammar[f\"A{i}\"] = rhs\n\n  def expand_non_terminal(self, non_terminal: str):\n      \"\"\"\n      Recursively expand a given non-terminal symbol by replacing it with its right-hand side symbols, which may themselves be non-terminals or terminals.\n      \"\"\"\n      # Retrieve the right-hand side of the specified non-terminal.\n      rhs_symbols = self.grammar[non_terminal]\n      string = \"\"\n\n      # Recursively expand each symbol in the right-hand side.\n      for symbol in rhs_symbols:\n          if symbol.startswith(\"A\"):\n              # If the symbol is a non-terminal, expand it further.\n              string += self.expand_non_terminal(symbol)\n          else:\n              # If the symbol is a terminal, append it directly.\n              string += symbol\n      return string\n\n\nGdec = GrammarDecoder(G.binary)\nGdec.decode()\n\nfor rule in Gdec.grammar.items():\n    print(rule)\n\n('A0', ['A1', 'A2', 'A3', 'A4', 'A5', 'A6'])\n('A1', ['a'])\n('A2', ['b'])\n('A3', ['A1', 'a'])\n('A4', ['A2', 'a'])\n('A5', ['A3', 'a'])\n('A6', ['A3', 'b'])\n\n\n\nGdec.expand_non_terminal(\"A0\")\n\n'abaabaaaaaab'\n\n\n\nGdec.expand_non_terminal(\"A0\") == s\n\nTrue"
  },
  {
    "objectID": "posts/2025-02-23-grammar_based_data_compression/index.html#thinking-exercises",
    "href": "posts/2025-02-23-grammar_based_data_compression/index.html#thinking-exercises",
    "title": "Grammar-based data compression",
    "section": "Thinking exercises",
    "text": "Thinking exercises\n\nExplain why the grammar encoder defined in this script is uniquely decodable.\nWrite code for token level compression, e.g. words or subwords.\nHow Kieffer-Yang’s encoder differs from local encoders?"
  },
  {
    "objectID": "posts/2025-02-23-grammar_based_data_compression/index.html#references",
    "href": "posts/2025-02-23-grammar_based_data_compression/index.html#references",
    "title": "Grammar-based data compression",
    "section": "References",
    "text": "References\n\nKieffer, J. C. & En-Hui Yang. (2000). Grammar-based codes: A new class of universal lossless source codes. IEEE Transactions on Information Theory, 46(3), 737–754. https://doi.org/10.1109/18.841160\nCover, T. M., & Thomas, J. A. (2006). Elements of Information Theory (2nd edition). Wiley-Interscience."
  },
  {
    "objectID": "posts/2025-02-07-skip_gram_from_scratch/index.html",
    "href": "posts/2025-02-07-skip_gram_from_scratch/index.html",
    "title": "Word2Vec skip-gram from scratch in R",
    "section": "",
    "text": "Word2Vec models were first presented in the paper “Efficient Estimation of Word Representations in Vector Space” by Google team (Mikolov et al. 2013). They presented two model architectures:\n\nContinuous Bag-of-Words Model aka CBOW, where given context words we predict target words\nContinuous Skip-gram Model aka skip-gram, where given target words we predict context words.\n\nThe goal is to find numerical representation of raw data, in our case those will be text tokens. This numerical representation should be a collection of dense vectors in \\(d\\)-dimensional Euclidean space.\nSince then embedding models have been widely used in the industry. For examples of use-cases we recommend an essay What are embeddings? by Vicki Boykis.\nIn this tutorial we’ll explore step-by-step how skip-gram model works accompanied by code in R language and a toy example.\nWe’ll use object-oriented approach with R6 interface (https://r6.r-lib.org/index.html). We’ll use torch library (https://torch.mlverse.org/) for automatic differentiation and GPU-accelerated matrix operations. As of February 2025, torch in R suuports CUDA v11.8. Below code was run on NVIDIA GeForce GTX 1650 Ti.\n\nlibrary(torch) # v0.13.0\nlibrary(R6)    # v2.5.1\n\n\nset_device = function() {\n  if (cuda_is_available()) {\n    device = torch_device(\"cuda\")\n    cat(\"Using GPU (CUDA)\\n\")\n  } else {\n    device = torch_device(\"cpu\")\n    cat(\"Using CPU\\n\")\n  }\n}"
  },
  {
    "objectID": "posts/2025-02-07-skip_gram_from_scratch/index.html#introduction",
    "href": "posts/2025-02-07-skip_gram_from_scratch/index.html#introduction",
    "title": "Word2Vec skip-gram from scratch in R",
    "section": "",
    "text": "Word2Vec models were first presented in the paper “Efficient Estimation of Word Representations in Vector Space” by Google team (Mikolov et al. 2013). They presented two model architectures:\n\nContinuous Bag-of-Words Model aka CBOW, where given context words we predict target words\nContinuous Skip-gram Model aka skip-gram, where given target words we predict context words.\n\nThe goal is to find numerical representation of raw data, in our case those will be text tokens. This numerical representation should be a collection of dense vectors in \\(d\\)-dimensional Euclidean space.\nSince then embedding models have been widely used in the industry. For examples of use-cases we recommend an essay What are embeddings? by Vicki Boykis.\nIn this tutorial we’ll explore step-by-step how skip-gram model works accompanied by code in R language and a toy example.\nWe’ll use object-oriented approach with R6 interface (https://r6.r-lib.org/index.html). We’ll use torch library (https://torch.mlverse.org/) for automatic differentiation and GPU-accelerated matrix operations. As of February 2025, torch in R suuports CUDA v11.8. Below code was run on NVIDIA GeForce GTX 1650 Ti.\n\nlibrary(torch) # v0.13.0\nlibrary(R6)    # v2.5.1\n\n\nset_device = function() {\n  if (cuda_is_available()) {\n    device = torch_device(\"cuda\")\n    cat(\"Using GPU (CUDA)\\n\")\n  } else {\n    device = torch_device(\"cpu\")\n    cat(\"Using CPU\\n\")\n  }\n}"
  },
  {
    "objectID": "posts/2025-02-07-skip_gram_from_scratch/index.html#skip-gram-workflow",
    "href": "posts/2025-02-07-skip_gram_from_scratch/index.html#skip-gram-workflow",
    "title": "Word2Vec skip-gram from scratch in R",
    "section": "Skip-gram workflow",
    "text": "Skip-gram workflow\nInput to our model is a sequence of \\(n\\) tokens \\(\\mathbf{x} = (x_1, x_2, \\dots, x_n)\\). We must also specify dimensionality \\(d\\) of a target embedding space. Thus a contructor to initialize our class is defined as follows.\n\ninitialize = function(tokens, embedding_dim) {\n  self$tokens = tokens\n  self$embedding_dim = embedding_dim\n}\n\n\nBuild vocabulary\nAmong \\(n\\) tokens there are \\(m \\leq n\\) unique tokens. Let’s denote our vocabulary\n\\[\nV = \\left(v_1, v_2, \\dots , v_m \\right)\n\\]\nand the set of corresponding indices \\(C = \\lbrace 1, 2, \\dots, m \\rbrace\\).\n\nbuild_vocabulary = function() {\n  unique_tokens = unique(self$tokens)\n  token_ids = seq_along(unique_tokens)\n  self$vocabulary = setNames(token_ids, unique_tokens)\n}\n\n\n\nConvert tokens to indices\nWe convert sequence of tokens \\(\\mathbf{x} = (x_1, x_2, \\dots, x_n)\\) to sequence of integers \\(( m_1, m_2, \\dots , m_n)\\) where \\(m_i \\in C\\) for \\(i = 1, 2, \\dots, n\\).\n\nconvert_tokens_to_indices = function() {\n  self$token_indices = sapply(self$tokens, function(token) {\n    self$vocabulary[[token]]\n  })\n}\n\n\n\nInitialize model weights\nWe need to initialize two matrices:\n\n\\(W_1\\) of dimensionality \\(m \\times d\\)\n\\(W_2\\) of dimensionality \\(d \\times m\\)\n\nMatrix \\(W_1\\) encodes hidden state which eventually we’ll interpret as the word embeddings of our vocabulary \\(V\\).\n\ninitialize_weights = function() {\n  vocabulary_size = length(self$vocabulary)\n  self$W1 = torch_randn(\n    size = c(vocabulary_size, self$embedding_dim)\n    , requires_grad = TRUE\n    , dtype = torch_float32()\n  )\n  self$W2 = torch_randn(\n    size = c(self$embedding_dim, vocabulary_size)\n    , requires_grad = TRUE\n    , dtype = torch_float32()\n  )\n}\n\n\n\nTrain model\nIn this tutorial we’ll find optimal model weights via vanilla gradient descent. As an exercise we recommend to try out this code with more efficient optimizers, e.g. SGD or Adam. For survey of different optimizers check out this video: https://www.youtube.com/watch?v=MD2fYip6QsQ\nIn order to train our model we must specify 3 additional parameters:\n\n\\(l\\) - window size for skip-gram method\nnumber of epochs during the training\n\\(\\eta\\) - learning rate for gradient descent.\n\nThere will be 3 loops:\n\nOuter loop for each epoch.\nThen for each token \\(x_i\\), \\(i =1,2,\\dots, n\\), we take the window \\([x_{i-l}, \\dots, x_{i-1}, x_i, x_{i+1}, \\dots x_{i+l}]\\). It gives us pairs of center words and context words \\((x_i, x_j)\\), \\(j \\in \\lbrace i-l, \\dots i-1, i+1, \\dots i+l \\rbrace\\).\nFor each pair \\((x_i, x_j)\\) we’ll perform forward pass, loss calculation and backpropagation.\n\n\nfor (epoch in seq_len(epochs)) {\n  total_loss = 0\n  \n  # Loop over each token position as the center word\n  for (center_position in seq_along(self$tokens)) {\n    center_word = self$tokens[[center_position]]\n    center_index   = self$vocabulary[[center_word]]\n          \n    # Determine context window boundaries\n    start_position = max(1, center_position - window_size)\n    end_posisiton   = min(length(self$tokens), center_position + window_size)\n          \n    # For each context word\n    for (context_position in seq(start_position, end_posisiton)) {\n      # If center position is equal to context position then skip to the next iteration\n      if (context_position == center_position) next\n      context_word = self$tokens[[context_position]]\n      context_index   = self$vocabulary[[context_word]]\n    \n      # FORWARD PASS\n      # CALCULATE LOSS\n      # BACKPROPAGATION\n  }}}\n\n\nForward pass\nWe project center word’s embedding \\(\\left( W_1 \\right)_{i\\cdot}\\) (i.e. \\(i\\)-th row of matrix \\(W_1\\)) onto column space of the matrix \\(W_2\\). It gives us vector \\(v_{ij}\\) of length \\(m\\). We turn this vector into probabilities via softmax function\n\\[\np(v_{ij}) = \\frac{\\exp(v_{ij})}{\\sum_{k=1}^m \\exp(v_{ik})}\n\\]\n\nforward_pass = function(center_word_index) {\n  hidden_layer = self$W1[center_word_index, ]\n  output_layer = hidden_layer$matmul(self$W2)\n  return(output_layer)\n}\n\n\n\nLoss calculation\nNow for the pair \\((x_i, x_j)\\) we have a vector of probabilities \\(p(v_{ij}) = (p_1, p_2, \\dots , p_k)\\). True label of the word \\(x_j\\) is one-hot vector \\(y_j\\) with \\(1\\) at \\(j\\)-th position and \\(0\\)’s elsewhere. Cross-entropy loss is then:\n\\[\nL(y_j, p(v_{ij})) = - \\sum_{k=1}^m (y_j)_k \\log_2 p_k = -\\log_2 p_j\n\\]\nTechnical note. torch::nnf_cross_entropy() function takes 2 arguments:\n\ninput tensor of shape (batch_size, size_of_vocabulary). Since we process each center word at a time, then in our case batch_size = 1, so we apply unsqueeze() function on the vector \\(v_{ij}\\).\ntarget tensor containing index \\(m_j \\in \\lbrace 1, 2, \\dots, m \\rbrace\\) of our context word \\(x_j\\).\n\nIn the backend, torch::nnf_cross_entropy() applies softmax on the input tensor and one-hot encodes target tensor.\n\ncompute_cross_entropy(output_layer, context_word_index) {\n  y_hat = output_layer$unsqueeze(dim = 1)\n  y_true = torch_tensor(context_word_index, dtype = torch_long())\n  loss = nnf_cross_entropy(y_hat, y_true)\n  return(loss)\n}\n\n\n\nBackpropagation\ntorch automatically calculates derivatives \\(\\nabla W_1\\), \\(\\nabla W_2\\) of the loss function \\(L\\) with respect to the model weights \\(W_1\\) and \\(W_2\\). Then we update matrices \\(W_1\\) and \\(W_2\\) in the direction of the steepest descent:\n\\[\nW \\leftarrow W - \\eta \\ \\nabla W\n\\]\n\nupdate_weights = function(loss, learning_rate) {\n  loss$backward()\n  \n  with_no_grad({\n    self$W1$sub_(learning_rate * self$W1$grad)\n    self$W2$sub_(learning_rate * self$W2$grad)\n    \n    # Zero out gradients for next iteration\n    self$W1$grad$zero_()\n    self$W2$grad$zero_()\n  })\n}"
  },
  {
    "objectID": "posts/2025-02-07-skip_gram_from_scratch/index.html#putting-it-all-together-into-r6-interface",
    "href": "posts/2025-02-07-skip_gram_from_scratch/index.html#putting-it-all-together-into-r6-interface",
    "title": "Word2Vec skip-gram from scratch in R",
    "section": "Putting it all together into R6 interface",
    "text": "Putting it all together into R6 interface\n\nWord2Vec = R6::R6Class(\n  classname = \"Word2Vec\",\n  \n  public = list(\n    # Attributes created during initialization\n    tokens = NULL,\n    embedding_dim = NULL,\n    \n    # Attributes created during runtime\n    device = NULL,\n    vocabulary = NULL,\n    token_indices = NULL,\n    W1 = NULL,\n    W2 = NULL,\n    \n    # Methods\n    initialize = function(tokens, embedding_dim) {\n      self$tokens = tokens\n      self$embedding_dim = embedding_dim\n    },\n    \n    set_device = function() {\n      if (cuda_is_available()) {\n        device = torch_device(\"cuda\")\n        cat(\"Using GPU (CUDA)\\n\")\n      } else {\n        device = torch_device(\"cpu\")\n        cat(\"Using CPU\\n\")\n      }\n    },\n    \n    build_vocabulary = function() {\n      unique_tokens = unique(self$tokens)\n      token_ids = seq_along(unique_tokens)\n      self$vocabulary = setNames(token_ids, unique_tokens)\n    },\n    \n    convert_tokens_to_indices = function() {\n      self$token_indices = sapply(self$tokens, function(token) {\n        self$vocabulary[[token]]\n      })\n    },\n    \n    initialize_weights = function() {\n      vocabulary_size = length(self$vocabulary)\n      self$W1 = torch_randn(\n        size = c(vocabulary_size, self$embedding_dim)\n        , requires_grad = TRUE\n        , dtype = torch_float32()\n        , device = self$device\n      )\n      self$W2 = torch_randn(\n        size = c(self$embedding_dim, vocabulary_size)\n        , requires_grad = TRUE\n        , dtype = torch_float32()\n        , device = self$device\n      )\n    },\n    \n    forward_pass = function(center_word_index) {\n      hidden_layer = self$W1[center_word_index, ]\n      output_layer = hidden_layer$matmul(self$W2)\n      return(output_layer)\n    },\n    \n    compute_cross_entropy = function(output_layer, context_word_index) {\n      y_hat = output_layer$unsqueeze(dim = 1)\n      y_true = torch_tensor(\n        context_word_index\n        , dtype = torch_long()\n        , device = self$device\n      )\n      loss = nnf_cross_entropy(y_hat, y_true)\n      return(loss)\n    },\n    \n    update_weights = function(loss, learning_rate) {\n      loss$backward()\n      \n      with_no_grad({\n        self$W1$sub_(learning_rate * self$W1$grad)\n        self$W2$sub_(learning_rate * self$W2$grad)\n        \n        # Zero out gradients for next iteration\n        self$W1$grad$zero_()\n        self$W2$grad$zero_()\n      })\n    },\n    \n    train_model = function(window_size, epochs, learning_rate) {\n      for (epoch in seq_len(epochs)) {\n      total_loss = 0\n      \n      # Loop over each token position as the center word\n      for (center_position in seq_along(self$tokens)) {\n        center_word = self$tokens[[center_position]]\n        center_index   = self$vocabulary[[center_word]]\n              \n        # Determine context window boundaries\n        start_position = max(1, center_position - window_size)\n        end_posisiton   = min(length(self$tokens), center_position + window_size)\n              \n        # For each context word\n        for (context_position in seq(start_position, end_posisiton)) {\n          # If center position is equal to context position then skip to the next iteration\n          if (context_position == center_position) next\n          context_word = self$tokens[[context_position]]\n          context_index   = self$vocabulary[[context_word]]\n                \n            # Forward pass\n            output_layer = self$forward_pass(center_index)\n            \n            # Compute loss\n            loss = self$compute_cross_entropy(output_layer, context_index)\n            total_loss = total_loss + as.numeric(loss$item())\n            \n            # Update params\n            self$update_weights(loss, learning_rate)\n          }\n        }\n        \n        cat(sprintf(\"Epoch %d/%d, Loss: %.4f\\n\", epoch, epochs, total_loss))\n      }\n    }\n    \n  )\n)"
  },
  {
    "objectID": "posts/2025-02-07-skip_gram_from_scratch/index.html#toy-example",
    "href": "posts/2025-02-07-skip_gram_from_scratch/index.html#toy-example",
    "title": "Word2Vec skip-gram from scratch in R",
    "section": "Toy example",
    "text": "Toy example\nLet’s craft 20 sentences corpus on which we’ll train our model (corpus generated by GPT o1).\n\ntext_corpus = \"My cat likes food. I give food to cat. Boy sees cat. Girl sees dog. Dog eats food. Cat sleeps now. Girl has cat. Boy feeds dog. Cat likes boy. Dog likes girl. Girl eats food. Boy runs home. Cat follows boy. Dog follows girl. Food is good. Cat is happy. Girl is happy. Boy is happy. Dog is hungry. They share food.\"\n\nWe apply some preprocessing steps:\n\nremove punctuation\nconvert text to lower-case\nsplit into tokens, where each token represent a word.\n\n\nprep_text = gsub(\"[[:punct:]]\", \"\", tolower(text_corpus))\ntokens = strsplit(prep_text, \"\\\\s+\") |&gt; unlist()\n\ntable(tokens) |&gt; as.data.frame()\n\n    tokens Freq\n1      boy    6\n2      cat    8\n3      dog    6\n4     eats    2\n5    feeds    1\n6  follows    2\n7     food    6\n8     girl    6\n9     give    1\n10    good    1\n11   happy    3\n12     has    1\n13    home    1\n14  hungry    1\n15       i    1\n16      is    5\n17   likes    3\n18      my    1\n19     now    1\n20    runs    1\n21    sees    2\n22   share    1\n23  sleeps    1\n24    they    1\n25      to    1\n\n\nIt gave us:\n\n\\(n  = 63\\) - number of tokens\n\\(m=25\\) - vocabulary size.\n\nNow we train our model with \\(d = 15\\), \\(l=1\\), \\(\\eta = 0.1\\) and \\(50\\) epochs.\n\n# set seed for reproducibility\nset.seed(1234)\ntorch_manual_seed(1234)\n\n# Create the model\nmodel = Word2Vec$new(tokens = tokens, embedding_dim = 15)\n\n# Run the model\nmodel$set_device()\n\nUsing GPU (CUDA)\n\nmodel$build_vocabulary()\nmodel$convert_tokens_to_indices()\nmodel$initialize_weights()\n\n# Train the model\nwindow_size  = 1   # each side\nepochs       = 50\nlearning_rate = 0.1\n\nmodel$train_model(window_size, epochs, learning_rate)\n\nEpoch 1/50, Loss: 746.5261\nEpoch 2/50, Loss: 447.8313\nEpoch 3/50, Loss: 383.7912\nEpoch 4/50, Loss: 356.2588\nEpoch 5/50, Loss: 341.8935\nEpoch 6/50, Loss: 333.6537\nEpoch 7/50, Loss: 327.7783\nEpoch 8/50, Loss: 323.0561\nEpoch 9/50, Loss: 319.0751\nEpoch 10/50, Loss: 315.6339\nEpoch 11/50, Loss: 312.6157\nEpoch 12/50, Loss: 309.9435\nEpoch 13/50, Loss: 307.5606\nEpoch 14/50, Loss: 305.4234\nEpoch 15/50, Loss: 303.4967\nEpoch 16/50, Loss: 301.7515\nEpoch 17/50, Loss: 300.1642\nEpoch 18/50, Loss: 298.7149\nEpoch 19/50, Loss: 297.3869\nEpoch 20/50, Loss: 296.1663\nEpoch 21/50, Loss: 295.0409\nEpoch 22/50, Loss: 294.0005\nEpoch 23/50, Loss: 293.0363\nEpoch 24/50, Loss: 292.1406\nEpoch 25/50, Loss: 291.3069\nEpoch 26/50, Loss: 290.5291\nEpoch 27/50, Loss: 289.8023\nEpoch 28/50, Loss: 289.1219\nEpoch 29/50, Loss: 288.4839\nEpoch 30/50, Loss: 287.8849\nEpoch 31/50, Loss: 287.3216\nEpoch 32/50, Loss: 286.7914\nEpoch 33/50, Loss: 286.2917\nEpoch 34/50, Loss: 285.8201\nEpoch 35/50, Loss: 285.3748\nEpoch 36/50, Loss: 284.9538\nEpoch 37/50, Loss: 284.5555\nEpoch 38/50, Loss: 284.1784\nEpoch 39/50, Loss: 283.8211\nEpoch 40/50, Loss: 283.4822\nEpoch 41/50, Loss: 283.1607\nEpoch 42/50, Loss: 282.8555\nEpoch 43/50, Loss: 282.5656\nEpoch 44/50, Loss: 282.2900\nEpoch 45/50, Loss: 282.0279\nEpoch 46/50, Loss: 281.7786\nEpoch 47/50, Loss: 281.5412\nEpoch 48/50, Loss: 281.3152\nEpoch 49/50, Loss: 281.0998\nEpoch 50/50, Loss: 280.8946\n\n\n\nPCA projection of embedding space\nOur final word embeddings are rows of the matrix \\(W_1\\) of dimensionality \\(25 \\times 15\\). We extract principal components of \\(W_1\\).\n\nembedding_space = model$W1 |&gt; as.matrix()\npca = prcomp(embedding_space, center = TRUE, scale. = TRUE)\nsummary(pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6     PC7\nStandard deviation     1.8047 1.5357 1.4169 1.18798 1.12567 1.01621 0.95311\nProportion of Variance 0.2171 0.1572 0.1338 0.09409 0.08448 0.06885 0.06056\nCumulative Proportion  0.2171 0.3743 0.5082 0.60228 0.68675 0.75560 0.81616\n                           PC8     PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     0.82368 0.72397 0.68359 0.65974 0.50026 0.46220 0.37462\nProportion of Variance 0.04523 0.03494 0.03115 0.02902 0.01668 0.01424 0.00936\nCumulative Proportion  0.86139 0.89633 0.92748 0.95650 0.97318 0.98743 0.99678\n                          PC15\nStandard deviation     0.21970\nProportion of Variance 0.00322\nCumulative Proportion  1.00000\n\n\nLet’s project embedding space into first two principal components which together explain \\(37.43\\%\\) of total variance.\n\nlibrary(ggplot2) # v3.5.1\n\npca_projection = embedding_space %*% pca$rotation[, 1:2]\nrownames(pca_projection) = names(model$vocabulary)\n\nggplot(data.frame(pca_projection), aes(x = PC1, y = PC2)) +\n  geom_point() +\n  geom_text(aes(label = rownames(pca_projection)), nudge_x = 0.1) +\n  theme_bw() +\n  labs(title = \"PCA Projection of Word Embeddings\")\n\n\n\n\n\n\n\n\nAs we can see, “similar” words are close each other:\n\nnouns boy and girl\nverbs sees, follows, runs, and likes\n\nHowever dog and cat are far apart. We might retrain the model with larger corpora, bigger embedding space, wider context window, more epochs, more robust optimizer (vanilla GD could be stuck in local minima). Altough words dog and sleeps, which are semantically related in case of my Labrador, are close each other."
  },
  {
    "objectID": "posts/2025-02-07-skip_gram_from_scratch/index.html#futher-steps",
    "href": "posts/2025-02-07-skip_gram_from_scratch/index.html#futher-steps",
    "title": "Word2Vec skip-gram from scratch in R",
    "section": "Futher steps",
    "text": "Futher steps\nIn practice, with huge vocabularies and enormous corpora, calculating denominator in the softmax function is quite expensive. To avoid that we use skip-gram with negative sampling aka SGNC, where having a pair (center word, context word) we pick randomly few context words as negative samples and fit binary logistic regression. For more details we refer to the chapter 6.8 of the book Speech and Language Processing by Daniel Jurafsky and James Martin."
  },
  {
    "objectID": "posts/2025-02-07-skip_gram_from_scratch/index.html#thinking-exercises",
    "href": "posts/2025-02-07-skip_gram_from_scratch/index.html#thinking-exercises",
    "title": "Word2Vec skip-gram from scratch in R",
    "section": "Thinking exercises",
    "text": "Thinking exercises\n\nIn fact, for each word \\(v_i\\) from the vocabulary \\(V\\) we have two word embeddings:\n\n\\(i\\)-th row of the matrix \\(W_1\\) where \\(v_i\\) served as a center word\n\\(i\\)-th column of the matrix \\(W_2\\) where \\(v_i\\) served as a context word.\n\nDo we get better word representation if combine (e.g. average) both vectors?\nSoftmax function has time complexity \\(\\mathcal{O}(n)\\). Replace it with hierarchical softmax which has time complexity \\(\\mathcal{O} (\\log n )\\)."
  },
  {
    "objectID": "posts/2025-02-07-skip_gram_from_scratch/index.html#references",
    "href": "posts/2025-02-07-skip_gram_from_scratch/index.html#references",
    "title": "Word2Vec skip-gram from scratch in R",
    "section": "References",
    "text": "References\n\nMikolov T. et al., Efficient Estimation of Word Representations in Vector Space, 2013, https://arxiv.org/abs/1301.3781\nBoykis V., What are embeddings?, https://vickiboykis.com/what_are_embeddings/\nChang W (2022). R6: Encapsulated Classes with Reference Semantics. https://r6.r-lib.org, https://github.com/r-lib/R6/.\nhttps://torch.mlverse.org/\nKundu S., Who’s Adam and What’s He Optimizing?, 2024, https://www.youtube.com/watch?v=MD2fYip6QsQ\nJurafsky D., Martin J. H., Speech and Language Processing, 2024, https://web.stanford.edu/~jurafsky/slp3/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "This is my blog where I share thoughts on different topics."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About Me",
    "section": "",
    "text": "This is my blog where I share thoughts on different topics."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pawel-wieczynski.github.io",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/2025-02-07-test_document/index.html",
    "href": "posts/2025-02-07-test_document/index.html",
    "title": "Test document",
    "section": "",
    "text": "This is example blog post to test if my Github Pages are configured correctly. Let’s plot formula of the function \\[\nf(x) = \\exp \\left( \\frac{1}{2}x \\right) + \\sin^2(x)\n\\]\n\n\n\n\nx = seq(-1, 1, 0.01)\ny = exp(x/2) + sin(x)^2\nplot(x, y, type = 'l')"
  },
  {
    "objectID": "posts/2025-02-07-test_document/index.html#test-document",
    "href": "posts/2025-02-07-test_document/index.html#test-document",
    "title": "Test document",
    "section": "",
    "text": "This is example blog post to test if my Github Pages are configured correctly. Let’s plot formula of the function \\[\nf(x) = \\exp \\left( \\frac{1}{2}x \\right) + \\sin^2(x)\n\\]\n\n\n\n\nx = seq(-1, 1, 0.01)\ny = exp(x/2) + sin(x)^2\nplot(x, y, type = 'l')"
  },
  {
    "objectID": "posts/2025-03-06-guassian_process_regression/index.html",
    "href": "posts/2025-03-06-guassian_process_regression/index.html",
    "title": "Gaussian Process Regression",
    "section": "",
    "text": "In most machine learning algorithms we are eventually predicting a number, either a real-valued output in a regression setting or a class membership probability in a classification setting. Gaussian Processes (GP’s), on the other hand, allows us to predict a distribution which gives us eventually not only an output value but also quantifies uncertainty related to that number.\nAnother advantage is that GP’s work well with variable length strings, which in particular can be useful in natural language processing (NLP).\nIn this blog post we will explore Gaussian Process Regression (GPR). First we will introduce a bit of theoretical background followed by an implementation of GPR from scratch. This part is based on chapter 2 from Rasmussen’s textbook.\nOne drawback is that GPR requires inverting kernel matrix which takes \\(\\mathcal{O}(n^3)\\) time, where \\(n\\) is the number of training samples. To partially overcome that, we will explore GPU-accelerated framework for GP’s GPyTorch. Another approach for scalable GP’s are sparse Gaussian processes, which are out-of-scope of this article, but I wish to explore it in the future."
  },
  {
    "objectID": "posts/2025-03-06-guassian_process_regression/index.html#hyperparameters",
    "href": "posts/2025-03-06-guassian_process_regression/index.html#hyperparameters",
    "title": "Gaussian Process Regression",
    "section": "Hyperparameters",
    "text": "Hyperparameters\nIn this blog post we will use radial-basis function (RBF) as a kernel, given by:\n\\[ k(\\mathbf{x}, \\mathbf{x}^\\prime) = \\sigma^2 \\exp \\left( - \\frac{||\\mathbf{x} - \\mathbf{x}^\\prime ||^2}{2 \\mathcal{l} ^2} \\right) \\]\nIn this setup we have 3 hyperparameters: \\(\\theta = (\\sigma^2, \\mathcal{l} , \\sigma^2_\\epsilon)\\) to be optimized. We achieve this by minimizing negative log marginal likelihood:\n\\[ -\\log p_f(\\mathbf{y} | X, \\theta) = -\\log \\left( \\int p(\\mathbf{y} | f(X)) p(f(X)|X, \\theta) df \\right) \\]\nWhen both \\(p(\\mathbf{y} | f(X))\\) and \\(p(f(X)|X, \\theta)\\) are Gaussians, then the above formula has a closed-form solution."
  },
  {
    "objectID": "posts/2025-03-06-guassian_process_regression/index.html#toy-dataset",
    "href": "posts/2025-03-06-guassian_process_regression/index.html#toy-dataset",
    "title": "Gaussian Process Regression",
    "section": "Toy dataset",
    "text": "Toy dataset\n\nimport numpy as np\nimport torch\nimport gpytorch\nimport matplotlib.pyplot as plt\n\nLet us assume that true underlying function is given by:\n\\[\nf(x) = \\cos(2x) + \\frac{1}{2}x, \\quad x \\in [0, 2\\pi]\n\\]\nOur training outcomes \\(y_i\\) are sampled from \\(f\\) with noise \\(\\epsilon \\sim \\mathcal{N} (0, 0.05)\\). In below example we have \\(n=10\\) training data points.\n\nx_max = 2*np.pi\ndef true_function(x):\n  return np.cos(2*x) + 0.5*x\n\nx = np.linspace(0, x_max, 1000)\ny = true_function(x)\n\nnp.random.seed(2136)\ntrain_x = np.random.rand(10) * x_max\ntrain_y = true_function(train_x) + np.random.randn(train_x.shape[0]) * np.sqrt(0.05)\n\nplt.figure()\nplt.plot(x, y, label = \"True function\")\nplt.scatter(train_x, train_y, c = \"#ff7f0e\", label = \"Training points\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/2025-03-06-guassian_process_regression/index.html#gpr-class-implementation",
    "href": "posts/2025-03-06-guassian_process_regression/index.html#gpr-class-implementation",
    "title": "Gaussian Process Regression",
    "section": "GPR class implementation",
    "text": "GPR class implementation\nIn below definition of RBF kernel we assume for simplicity that \\(\\sigma^2 = 1\\), \\(\\mathcal{l} = 1 / \\sqrt{2}\\), \\(\\sigma^2\\epsilon = 0\\). Later we will optimize them.\n\ndef gaussian_kernel(x1, x2):\n    return np.exp(-(x1 - x2)**2)\n\nWe initialize class GPR with training samples (optionally we can specify amount of noise \\(\\sigma^2_\\epsilon\\)). During the initialization \\(K\\) and \\((K + \\sigma^2_\\epsilon )^{-1}\\) are derived and stored in memory.\nMethod predict() returns mean and variance of a posterior distribution evaluated on a test point \\(x^*\\). Method make_plots() creates a plot with true function \\(f(x)\\), training points \\((x_i, y_i)_{i=1}^n\\) and predictions on test data \\((x_j^*, y_j)_{j=1}^m\\) with shaded areas for mean predictions \\(\\pm 1- 3\\) standard deviations.\n\nclass GPR:\n  def __init__(self, x, y, noise = 0):\n    self.x = x\n    self.y = y\n    self.noise = noise\n    # Initialize kernel matrix of training dataset\n    self.K = np.asarray([[gaussian_kernel(x1, x2) for x1 in self.x] for x2 in self.x])\n    self.K_inverse = np.linalg.inv(self.K + self.noise)\n    \n  def predict(self, x_new):\n    k_star = np.array([gaussian_kernel(x1, x_new) for x1 in self.x])\n    f_new = k_star.T @ self.K_inverse @ self.y\n    f_var = gaussian_kernel(x_new, x_new) - k_star.T @ self.K_inverse @ k_star\n    return f_new, f_var\n  \n  def make_plot(self, x_true, y_true, predictions, variances):\n    plt.figure()\n    plt.fill_between(x_true, predictions + 2*np.sqrt(variances), predictions + 3*np.sqrt(variances), color = 'grey', alpha = 0.10)\n    plt.fill_between(x_true, predictions - 2*np.sqrt(variances), predictions - 3*np.sqrt(variances), color = 'grey', alpha = 0.10)\n    plt.fill_between(x_true, predictions + 1*np.sqrt(variances), predictions + 2*np.sqrt(variances), color = 'grey', alpha = 0.30)\n    plt.fill_between(x_true, predictions - 1*np.sqrt(variances), predictions - 2*np.sqrt(variances), color = 'grey', alpha = 0.30)\n    plt.fill_between(x_true, predictions, predictions + np.sqrt(variances), color = 'grey', alpha = 0.50)\n    plt.fill_between(x_true, predictions, predictions - np.sqrt(variances), color = 'grey', alpha = 0.50)\n    plt.plot(x_true, y_true, label = \"True function\")\n    plt.plot(x_true, predictions, ls = \"--\", label = \"GPR\")\n    plt.scatter(self.x, self.y, marker = \"D\", s = 50, label = \"Training points\")\n    plt.legend()\n    plt.show()\n\n\nmodel = GPR(train_x, train_y)\npredictions = []\nvariances = []\n\nfor x_ in x:\n  f_new, f_var = model.predict(x_)\n  predictions.append(f_new)\n  variances.append(f_var)\n\n\nmodel.make_plot(x, y, predictions, np.sqrt(variances))\n\n\n\n\n\n\n\n\nAs we can see in the above plot, without hyperparameters optimization, GPR doesn’t yield good approximation of the true function \\(f(x)\\)."
  },
  {
    "objectID": "posts/2025-05-11-creating_llm_corpus/index.html",
    "href": "posts/2025-05-11-creating_llm_corpus/index.html",
    "title": "Building an LLM-generated text corpus for statistical analysis",
    "section": "",
    "text": "Texts in human languages exhibit fascinating statistical patterns. Many of these patterns persist across diverse languages, suggesting they reflect universal cognitive phenomena in human brains. One of the most famous examples is Zipf’s law, which states that the frequency of a word is inversely proportional to its rank in the frequency table.\nWith the increasing popularity of large language models (LLMs), it’s worth investigating whether texts generated by these models exhibit similar statistical patterns to human-written texts. Such comparisons could:\n\nReveal how well LLMs capture the statistical properties of human languages\nHighlight if LLMs generate text in fundamentally different ways than humans do\nEnable the development of tools for detecting artificially generated content\nShow variations in statistical properties based on model architecture, temperature settings, tokenization methods, and other factors\n\nIn this blog post, we’ll create a methodology for building a corpus of LLM-generated texts, which will serve as the foundation for future statistical analyses.\n\nimport sqlite3\nimport pandas as pd\nimport ollama\nimport openai\nimport os\nfrom dotenv import load_dotenv"
  },
  {
    "objectID": "posts/2025-05-11-creating_llm_corpus/index.html#introduction",
    "href": "posts/2025-05-11-creating_llm_corpus/index.html#introduction",
    "title": "Building an LLM-generated text corpus for statistical analysis",
    "section": "",
    "text": "Texts in human languages exhibit fascinating statistical patterns. Many of these patterns persist across diverse languages, suggesting they reflect universal cognitive phenomena in human brains. One of the most famous examples is Zipf’s law, which states that the frequency of a word is inversely proportional to its rank in the frequency table.\nWith the increasing popularity of large language models (LLMs), it’s worth investigating whether texts generated by these models exhibit similar statistical patterns to human-written texts. Such comparisons could:\n\nReveal how well LLMs capture the statistical properties of human languages\nHighlight if LLMs generate text in fundamentally different ways than humans do\nEnable the development of tools for detecting artificially generated content\nShow variations in statistical properties based on model architecture, temperature settings, tokenization methods, and other factors\n\nIn this blog post, we’ll create a methodology for building a corpus of LLM-generated texts, which will serve as the foundation for future statistical analyses.\n\nimport sqlite3\nimport pandas as pd\nimport ollama\nimport openai\nimport os\nfrom dotenv import load_dotenv"
  },
  {
    "objectID": "posts/2025-05-11-creating_llm_corpus/index.html#corpus-design",
    "href": "posts/2025-05-11-creating_llm_corpus/index.html#corpus-design",
    "title": "Building an LLM-generated text corpus for statistical analysis",
    "section": "Corpus design",
    "text": "Corpus design\nTo perform meaningful statistical analysis, we need a well-structured corpus of texts generated by different LLMs. Our corpus should:\n\nInclude texts from multiple LLM providers (Ollama, OpenAI, etc.)\nCover diverse topics and writing styles\nAllow for controlled experiments by varying parameters like temperature\nBe stored in a way that facilitates easy retrieval and analysis\n\nWe’ll use SQLite to store our corpus, as it provides a lightweight database solution that doesn’t require a separate server.\n\n# Connect to SQLite database (will be created if the file doesn't exist)\nconn = sqlite3.connect('llm_corpus.db')\ncursor = conn.cursor()\n\n# Create table for prompts\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS prompts (\n    id INTEGER PRIMARY KEY,\n    prompt TEXT NOT NULL\n)\n''')\n\n# Create table for generated texts with metadata\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS texts (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    text TEXT NOT NULL,\n    model_name TEXT NOT NULL,\n    temperature REAL NOT NULL,\n    prompt_id INTEGER NOT NULL,\n    system_prompt TEXT,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (prompt_id) REFERENCES prompts (id)\n)\n''')\n\n# Commit the schema changes\nconn.commit()"
  },
  {
    "objectID": "posts/2025-05-11-creating_llm_corpus/index.html#designing-diverse-text-prompts",
    "href": "posts/2025-05-11-creating_llm_corpus/index.html#designing-diverse-text-prompts",
    "title": "Building an LLM-generated text corpus for statistical analysis",
    "section": "Designing diverse text prompts",
    "text": "Designing diverse text prompts\nA diverse set of prompts will help us generate a representative corpus that captures different aspects of language use. To avoid biasing our corpus with our own preferences, we used a metaprompt approach to generate prompt ideas. We asked Perplexity AI to generate a list of diverse themes for our text corpus. The metaprompt was:\nI want to create a corpus of texts generated by different large language models. For this purpose I need a set of initial prompts. One example is \"Write a long story about a dragon and a princess.\". Give me several other examples, so I will have a thematically diversified set of prompts.\nThis meta-prompting approach yielded 15 diverse prompts:\n\nWrite a short horror story about a haunted house and the people who dare to enter it\nImagine you are a travel blogger. Write a post about your visit to a remote island in the Pacific.\nDiscuss the meaning of happiness and whether it is something that can be pursued or if it is a byproduct of other actions.\nWrite about a traditional festival from a culture other than your own, explaining its significance and customs.\nDescribe how artificial intelligence is likely to impact healthcare in the next decade.\nProvide step-by-step instructions on how to bake a chocolate cake from scratch.\nWrite a conversation between two friends who are debating whether to move to a new city.\nArgue for or against the statement: “Social media does more harm than good to society.”\nReflect on a moment in your life when you learned an important lesson. What happened, and how did it change you?\nWrite a poem about the changing of the seasons, focusing on the transition from autumn to winter.\nExplain the process of photosynthesis in simple terms suitable for a 10-year-old.\nCreate a detailed description of a magical forest and the creatures that inhabit it.\nImagine you are a soldier during the American Civil War. Write a letter home describing your experiences.\nDescribe a day in the life of an astronaut living on a space station orbiting Jupiter.\nWrite a long story about a detective solving a mystery in a futuristic city.\n\nThese prompts cover various genres (fiction, non-fiction, educational), formats (stories, essays, dialogues, poems), and topics, providing a good foundation for our corpus.\n\n# Import prompts from CSV file and store them in the database\ndef store_prompt(prompt):\n    \"\"\"Store a single prompt in the database and return its ID\"\"\"\n    cursor.execute('INSERT INTO prompts (prompt) VALUES (?)', (prompt,))\n    conn.commit()\n    return cursor.lastrowid\n\nprompts_df = pd.read_csv(\"initial_prompts.csv\", sep=\";\", header=None, names=[\"id\", \"prompt\"])\n\nfor _, row in prompts_df.iterrows():\n    prompt = row['prompt']\n    store_prompt(prompt)\n    \nprint(f\"Stored {len(prompts_df)} prompts in the database.\")"
  },
  {
    "objectID": "posts/2025-05-11-creating_llm_corpus/index.html#building-a-text-generation-framework",
    "href": "posts/2025-05-11-creating_llm_corpus/index.html#building-a-text-generation-framework",
    "title": "Building an LLM-generated text corpus for statistical analysis",
    "section": "Building a text generation framework",
    "text": "Building a text generation framework\nTo systematically collect texts from different LLMs, we need a unified interface that can work with multiple model providers. We’ll create a TextGenerator class that abstracts away the differences between model APIs and provides consistent functionality.\nOur framework needs to: 1. Support multiple LLM providers (Ollama, OpenAI, etc.) 2. Allow customization of generation parameters (temperature, system prompts) 3. Provide a consistent interface for text generation 4. Handle API-specific requirements and error cases\n\nclass TextGenerator:\n    \"\"\"\n    A class to generate text using various LLM providers with a unified interface.\n    \n    This class abstracts away the differences between different LLM APIs,\n    providing a consistent way to generate text regardless of the underlying model.\n    \"\"\"\n    def __init__(self, model_name, temperature=0.7, provider=\"ollama\"):\n        \"\"\"\n        Initialize the text generator\n        \n        Parameters:\n        -----------\n        model_name : str\n            Name of the model to use (e.g., \"gemma3:1b\", \"gpt-3.5-turbo\")\n        temperature : float\n            Temperature parameter for controlling randomness in text generation\n            Higher values (e.g., 0.8) produce more diverse outputs\n            Lower values (e.g., 0.2) produce more deterministic outputs\n        provider : str\n            Provider of the model (\"ollama\" or \"openai\")\n        \"\"\"\n        self.model_name = model_name\n        self.temperature = temperature\n        self.provider = provider\n        \n        # Configure provider-specific clients\n        if provider == \"openai\":\n            load_dotenv()  # Load API keys from .env file\n            self.client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n    \n    def generate_text(self, prompt, system_prompt=None):\n        \"\"\"\n        Generate text based on the given prompt\n        \n        Parameters:\n        -----------\n        prompt : str\n            The prompt to generate text from\n        system_prompt : str, optional\n            System prompt to guide the model's behavior\n            \n        Returns:\n        --------\n        str\n            Generated text\n        \"\"\"\n        if self.provider == \"ollama\":\n            response = ollama.generate(\n                model=self.model_name,\n                prompt=prompt,\n                system=system_prompt if system_prompt else \"\",\n                options={\"temperature\": self.temperature}\n            )\n            return response['response']\n            \n        elif self.provider == \"openai\":\n            messages = []\n            if system_prompt:\n                messages.append({\"role\": \"system\", \"content\": system_prompt})\n            messages.append({\"role\": \"user\", \"content\": prompt})\n            \n            response = self.client.chat.completions.create(\n                model=self.model_name,\n                messages=messages,\n                temperature=self.temperature\n            )\n            return response.choices[0].message.content\n        \n        else:\n            raise ValueError(f\"Unsupported provider: {self.provider}\")\n\n\ndef generate_and_store_texts(generator, prompts=None, system_prompt=None):\n    \"\"\"\n    Generate texts for all prompts using the provided text generator and store them in the database\n    \n    Parameters:\n    -----------\n    generator : TextGenerator\n        The text generator instance to use\n    prompts : list, optional\n        List of prompts to use. If None, fetches all prompts from the database.\n    system_prompt : str, optional\n        System prompt to guide the model's behavior\n        \n    Returns:\n    --------\n    int\n        Number of texts generated\n    \"\"\"\n    # Fetch prompts if not provided\n    if prompts is None:\n        cursor.execute('SELECT * FROM prompts')\n        prompts = cursor.fetchall()\n    \n    count = 0\n    for prompt in prompts:\n        prompt_id = prompt[0]\n        prompt_text = prompt[1]\n        \n        # Generate text using the provided generator\n        generated_text = generator.generate_text(prompt_text, system_prompt)\n        \n        # Store the generated text with metadata\n        cursor.execute('''\n        INSERT INTO texts (text, model_name, temperature, prompt_id, system_prompt)\n        VALUES (?, ?, ?, ?, ?)\n        ''', (generated_text, generator.model_name, generator.temperature, prompt_id, system_prompt))\n        \n        # Commit after each text to avoid losing progress if interrupted\n        conn.commit()\n        \n        count += 1\n    \n    return count"
  },
  {
    "objectID": "posts/2025-05-11-creating_llm_corpus/index.html#corpus-generation-process",
    "href": "posts/2025-05-11-creating_llm_corpus/index.html#corpus-generation-process",
    "title": "Building an LLM-generated text corpus for statistical analysis",
    "section": "Corpus generation process",
    "text": "Corpus generation process\nNow that we have our database schema and text generation framework in place, we can start building our corpus. We’ll generate texts using different models and store them in our database for later analysis.\n\n# Retrieve all prompts from the database\ncursor.execute('SELECT * FROM prompts')\nprompts = cursor.fetchall()\nprint(f\"Retrieved {len(prompts)} prompts from the database.\")\n\nWe will create system prompt to guide behaviour of the LLMs. Without it, for instance every answer from gemma3 model begins like “Okay, here’s a short story about…”.\n\nsystem_prompt = \"Please ONLY the answer to the prompt. Do not include any other text.\"\n\n\nGenerating texts with local models (ollama)\nOllama allows us to run open-source models locally. This is particularly useful for: - Preserving privacy by keeping generated data local - Avoiding API costs associated with cloud-based solutions - Testing smaller or specialized models not available via API services\nFor this example, we’ll use gemma3 (1B parameter version), a recent lightweight model from Google.\n\n# Start the Ollama model\nollama run gemma3:1b\n\n\nollama_model = TextGenerator(model_name=\"gemma3:1b\", provider=\"ollama\")\n\nfor i in range(1):\n    generate_and_store_texts(ollama_model, prompts, system_prompt)\n\n\n# Stop the Ollama\nollama stop gemma3:1b\n\n\n\nGenerating texts via API\nCloud-based models like those from OpenAI often provide state-of-the-art capabilities but require API access. Using these models allows us to: - Test cutting-edge models with greater capabilities - Compare commercial models with open-source alternatives - Examine differences between model families and architectures\nFor our cloud-based generation, we’ll use GPT-4o mini from OpenAI.\n\nopenai_model = TextGenerator(model_name=\"gpt-4o-mini-2024-07-18\", provider=\"openai\")\n\nfor i in range(1):\n    generate_and_store_texts(openai_model, prompts, system_prompt)"
  },
  {
    "objectID": "posts/2025-05-11-creating_llm_corpus/index.html#references",
    "href": "posts/2025-05-11-creating_llm_corpus/index.html#references",
    "title": "Building an LLM-generated text corpus for statistical analysis",
    "section": "References",
    "text": "References\nThis blog post was written with the assistance of the Claude 3.7 Snonnet Thinking model."
  }
]