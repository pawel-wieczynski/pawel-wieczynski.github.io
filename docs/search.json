[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nWord2Vec skip-gram from scratch in R\n\n\n\n\n\n\nNLP\n\n\ntorch\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\nPaweł Wieczyński\n\n\n\n\n\n\n\n\n\n\n\n\nTest document\n\n\n\n\n\n\nblog\n\n\ntest\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\nPaweł Wieczyński\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-02-07-skip_gram_from_scratch/index.html",
    "href": "posts/2025-02-07-skip_gram_from_scratch/index.html",
    "title": "Word2Vec skip-gram from scratch in R",
    "section": "",
    "text": "Word2Vec models were first presented in the paper “Efficient Estimation of Word Representations in Vector Space” by Google team (Mikolov et al. 2013). They presented two model architectures:\n\nContinuous Bag-of-Words Model aka CBOW, where given context words we predict target words\nContinuous Skip-gram Model aka skip-gram, where given target words we predict context words.\n\nThe goal is to find numerical representation of raw data, in our case those will be text tokens. This numerical representation should be a collection of dense vectors in \\(d\\)-dimensional Euclidean space.\nSince then embedding models have been widely used in the industry. For examples of use-cases we recommend an essay What are embeddings? by Vicki Boykis.\nIn this tutorial we’ll explore step-by-step how skip-gram model works accompanied by code in R language and a toy example.\nWe’ll use object-oriented approach with R6 interface (https://r6.r-lib.org/index.html). We’ll use torch library (https://torch.mlverse.org/) for automatic differentiation and GPU-accelerated matrix operations. As of February 2025, torch in R suuports CUDA v11.8. Below code was run on NVIDIA GeForce GTX 1650 Ti.\n\nlibrary(torch) # v0.13.0\nlibrary(R6)    # v2.5.1\n\n\nset_device = function() {\n  if (cuda_is_available()) {\n    device = torch_device(\"cuda\")\n    cat(\"Using GPU (CUDA)\\n\")\n  } else {\n    device = torch_device(\"cpu\")\n    cat(\"Using CPU\\n\")\n  }\n}"
  },
  {
    "objectID": "posts/2025-02-07-skip_gram_from_scratch/index.html#introduction",
    "href": "posts/2025-02-07-skip_gram_from_scratch/index.html#introduction",
    "title": "Word2Vec skip-gram from scratch in R",
    "section": "",
    "text": "Word2Vec models were first presented in the paper “Efficient Estimation of Word Representations in Vector Space” by Google team (Mikolov et al. 2013). They presented two model architectures:\n\nContinuous Bag-of-Words Model aka CBOW, where given context words we predict target words\nContinuous Skip-gram Model aka skip-gram, where given target words we predict context words.\n\nThe goal is to find numerical representation of raw data, in our case those will be text tokens. This numerical representation should be a collection of dense vectors in \\(d\\)-dimensional Euclidean space.\nSince then embedding models have been widely used in the industry. For examples of use-cases we recommend an essay What are embeddings? by Vicki Boykis.\nIn this tutorial we’ll explore step-by-step how skip-gram model works accompanied by code in R language and a toy example.\nWe’ll use object-oriented approach with R6 interface (https://r6.r-lib.org/index.html). We’ll use torch library (https://torch.mlverse.org/) for automatic differentiation and GPU-accelerated matrix operations. As of February 2025, torch in R suuports CUDA v11.8. Below code was run on NVIDIA GeForce GTX 1650 Ti.\n\nlibrary(torch) # v0.13.0\nlibrary(R6)    # v2.5.1\n\n\nset_device = function() {\n  if (cuda_is_available()) {\n    device = torch_device(\"cuda\")\n    cat(\"Using GPU (CUDA)\\n\")\n  } else {\n    device = torch_device(\"cpu\")\n    cat(\"Using CPU\\n\")\n  }\n}"
  },
  {
    "objectID": "posts/2025-02-07-skip_gram_from_scratch/index.html#skip-gram-workflow",
    "href": "posts/2025-02-07-skip_gram_from_scratch/index.html#skip-gram-workflow",
    "title": "Word2Vec skip-gram from scratch in R",
    "section": "Skip-gram workflow",
    "text": "Skip-gram workflow\nInput to our model is a sequence of \\(n\\) tokens \\(\\mathbf{x} = (x_1, x_2, \\dots, x_n)\\). We must also specify dimensionality \\(d\\) of a target embedding space. Thus a contructor to initialize our class is defined as follows.\n\ninitialize = function(tokens, embedding_dim) {\n  self$tokens = tokens\n  self$embedding_dim = embedding_dim\n}\n\n\nBuild vocabulary\nAmong \\(n\\) tokens there are \\(m \\leq n\\) unique tokens. Let’s denote our vocabulary\n\\[\nV = \\left(v_1, v_2, \\dots , v_m \\right)\n\\]\nand the set of corresponding indices \\(C = \\lbrace 1, 2, \\dots, m \\rbrace\\).\n\nbuild_vocabulary = function() {\n  unique_tokens = unique(self$tokens)\n  token_ids = seq_along(unique_tokens)\n  self$vocabulary = setNames(token_ids, unique_tokens)\n}\n\n\n\nConvert tokens to indices\nWe convert sequence of tokens \\(\\mathbf{x} = (x_1, x_2, \\dots, x_n)\\) to sequence of integers \\(( m_1, m_2, \\dots , m_n)\\) where \\(m_i \\in C\\) for \\(i = 1, 2, \\dots, n\\).\n\nconvert_tokens_to_indices = function() {\n  self$token_indices = sapply(self$tokens, function(token) {\n    self$vocabulary[[token]]\n  })\n}\n\n\n\nInitialize model weights\nWe need to initialize two matrices:\n\n\\(W_1\\) of dimensionality \\(m \\times d\\)\n\\(W_2\\) of dimensionality \\(d \\times m\\)\n\nMatrix \\(W_1\\) encodes hidden state which eventually we’ll interpret as the word embeddings of our vocabulary \\(V\\).\n\ninitialize_weights = function() {\n  vocabulary_size = length(self$vocabulary)\n  self$W1 = torch_randn(\n    size = c(vocabulary_size, self$embedding_dim)\n    , requires_grad = TRUE\n    , dtype = torch_float32()\n  )\n  self$W2 = torch_randn(\n    size = c(self$embedding_dim, vocabulary_size)\n    , requires_grad = TRUE\n    , dtype = torch_float32()\n  )\n}\n\n\n\nTrain model\nIn this tutorial we’ll find optimal model weights via vanilla gradient descent. As an exercise we recommend to try out this code with more efficient optimizers, e.g. SGD or Adam. For survey of different optimizers check out this video: https://www.youtube.com/watch?v=MD2fYip6QsQ\nIn order to train our model we must specify 3 additional parameters:\n\n\\(l\\) - window size for skip-gram method\nnumber of epochs during the training\n\\(\\eta\\) - learning rate for gradient descent.\n\nThere will be 3 loops:\n\nOuter loop for each epoch.\nThen for each token \\(x_i\\), \\(i =1,2,\\dots, n\\), we take the window \\([x_{i-l}, \\dots, x_{i-1}, x_i, x_{i+1}, \\dots x_{i+l}]\\). It gives us pairs of center words and context words \\((x_i, x_j)\\), \\(j \\in \\lbrace i-l, \\dots i-1, i+1, \\dots i+l \\rbrace\\).\nFor each pair \\((x_i, x_j)\\) we’ll perform forward pass, loss calculation and backpropagation.\n\n\nfor (epoch in seq_len(epochs)) {\n  total_loss = 0\n  \n  # Loop over each token position as the center word\n  for (center_position in seq_along(self$tokens)) {\n    center_word = self$tokens[[center_position]]\n    center_index   = self$vocabulary[[center_word]]\n          \n    # Determine context window boundaries\n    start_position = max(1, center_position - window_size)\n    end_posisiton   = min(length(self$tokens), center_position + window_size)\n          \n    # For each context word\n    for (context_position in seq(start_position, end_posisiton)) {\n      # If center position is equal to context position then skip to the next iteration\n      if (context_position == center_position) next\n      context_word = self$tokens[[context_position]]\n      context_index   = self$vocabulary[[context_word]]\n    \n      # FORWARD PASS\n      # CALCULATE LOSS\n      # BACKPROPAGATION\n  }}}\n\n\nForward pass\nWe project center word’s embedding \\(\\left( W_1 \\right)_{i\\cdot}\\) (i.e. \\(i\\)-th row of matrix \\(W_1\\)) onto column space of the matrix \\(W_2\\). It gives us vector \\(v_{ij}\\) of length \\(m\\). We turn this vector into probabilities via softmax function\n\\[\np(v_{ij}) = \\frac{\\exp(v_{ij})}{\\sum_{k=1}^m \\exp(v_{ik})}\n\\]\n\nforward_pass = function(center_word_index) {\n  hidden_layer = self$W1[center_word_index, ]\n  output_layer = hidden_layer$matmul(self$W2)\n  return(output_layer)\n}\n\n\n\nLoss calculation\nNow for the pair \\((x_i, x_j)\\) we have a vector of probabilities \\(p(v_{ij}) = (p_1, p_2, \\dots , p_k)\\). True label of the word \\(x_j\\) is one-hot vector \\(y_j\\) with \\(1\\) at \\(j\\)-th position and \\(0\\)’s elsewhere. Cross-entropy loss is then:\n\\[\nL(y_j, p(v_{ij})) = - \\sum_{k=1}^m (y_j)_k \\log_2 p_k = -\\log_2 p_j\n\\]\nTechnical note. torch::nnf_cross_entropy() function takes 2 arguments:\n\ninput tensor of shape (batch_size, size_of_vocabulary). Since we process each center word at a time, then in our case batch_size = 1, so we apply unsqueeze() function on the vector \\(v_{ij}\\).\ntarget tensor containing index \\(m_j \\in \\lbrace 1, 2, \\dots, m \\rbrace\\) of our context word \\(x_j\\).\n\nIn the backend, torch::nnf_cross_entropy() applies softmax on the input tensor and one-hot encodes target tensor.\n\ncompute_cross_entropy(output_layer, context_word_index) {\n  y_hat = output_layer$unsqueeze(dim = 1)\n  y_true = torch_tensor(context_word_index, dtype = torch_long())\n  loss = nnf_cross_entropy(y_hat, y_true)\n  return(loss)\n}\n\n\n\nBackpropagation\ntorch automatically calculates derivatives \\(\\nabla W_1\\), \\(\\nabla W_2\\) of the loss function \\(L\\) with respect to the model weights \\(W_1\\) and \\(W_2\\). Then we update matrices \\(W_1\\) and \\(W_2\\) in the direction of the steepest descent:\n\\[\nW \\leftarrow W - \\eta \\ \\nabla W\n\\]\n\nupdate_weights = function(loss, learning_rate) {\n  loss$backward()\n  \n  with_no_grad({\n    self$W1$sub_(learning_rate * self$W1$grad)\n    self$W2$sub_(learning_rate * self$W2$grad)\n    \n    # Zero out gradients for next iteration\n    self$W1$grad$zero_()\n    self$W2$grad$zero_()\n  })\n}"
  },
  {
    "objectID": "posts/2025-02-07-skip_gram_from_scratch/index.html#putting-it-all-together-into-r6-interface",
    "href": "posts/2025-02-07-skip_gram_from_scratch/index.html#putting-it-all-together-into-r6-interface",
    "title": "Word2Vec skip-gram from scratch in R",
    "section": "Putting it all together into R6 interface",
    "text": "Putting it all together into R6 interface\n\nWord2Vec = R6::R6Class(\n  classname = \"Word2Vec\",\n  \n  public = list(\n    # Attributes created during initialization\n    tokens = NULL,\n    embedding_dim = NULL,\n    \n    # Attributes created during runtime\n    device = NULL,\n    vocabulary = NULL,\n    token_indices = NULL,\n    W1 = NULL,\n    W2 = NULL,\n    \n    # Methods\n    initialize = function(tokens, embedding_dim) {\n      self$tokens = tokens\n      self$embedding_dim = embedding_dim\n    },\n    \n    set_device = function() {\n      if (cuda_is_available()) {\n        device = torch_device(\"cuda\")\n        cat(\"Using GPU (CUDA)\\n\")\n      } else {\n        device = torch_device(\"cpu\")\n        cat(\"Using CPU\\n\")\n      }\n    },\n    \n    build_vocabulary = function() {\n      unique_tokens = unique(self$tokens)\n      token_ids = seq_along(unique_tokens)\n      self$vocabulary = setNames(token_ids, unique_tokens)\n    },\n    \n    convert_tokens_to_indices = function() {\n      self$token_indices = sapply(self$tokens, function(token) {\n        self$vocabulary[[token]]\n      })\n    },\n    \n    initialize_weights = function() {\n      vocabulary_size = length(self$vocabulary)\n      self$W1 = torch_randn(\n        size = c(vocabulary_size, self$embedding_dim)\n        , requires_grad = TRUE\n        , dtype = torch_float32()\n        , device = self$device\n      )\n      self$W2 = torch_randn(\n        size = c(self$embedding_dim, vocabulary_size)\n        , requires_grad = TRUE\n        , dtype = torch_float32()\n        , device = self$device\n      )\n    },\n    \n    forward_pass = function(center_word_index) {\n      hidden_layer = self$W1[center_word_index, ]\n      output_layer = hidden_layer$matmul(self$W2)\n      return(output_layer)\n    },\n    \n    compute_cross_entropy = function(output_layer, context_word_index) {\n      y_hat = output_layer$unsqueeze(dim = 1)\n      y_true = torch_tensor(\n        context_word_index\n        , dtype = torch_long()\n        , device = self$device\n      )\n      loss = nnf_cross_entropy(y_hat, y_true)\n      return(loss)\n    },\n    \n    update_weights = function(loss, learning_rate) {\n      loss$backward()\n      \n      with_no_grad({\n        self$W1$sub_(learning_rate * self$W1$grad)\n        self$W2$sub_(learning_rate * self$W2$grad)\n        \n        # Zero out gradients for next iteration\n        self$W1$grad$zero_()\n        self$W2$grad$zero_()\n      })\n    },\n    \n    train_model = function(window_size, epochs, learning_rate) {\n      for (epoch in seq_len(epochs)) {\n      total_loss = 0\n      \n      # Loop over each token position as the center word\n      for (center_position in seq_along(self$tokens)) {\n        center_word = self$tokens[[center_position]]\n        center_index   = self$vocabulary[[center_word]]\n              \n        # Determine context window boundaries\n        start_position = max(1, center_position - window_size)\n        end_posisiton   = min(length(self$tokens), center_position + window_size)\n              \n        # For each context word\n        for (context_position in seq(start_position, end_posisiton)) {\n          # If center position is equal to context position then skip to the next iteration\n          if (context_position == center_position) next\n          context_word = self$tokens[[context_position]]\n          context_index   = self$vocabulary[[context_word]]\n                \n            # Forward pass\n            output_layer = self$forward_pass(center_index)\n            \n            # Compute loss\n            loss = self$compute_cross_entropy(output_layer, context_index)\n            total_loss = total_loss + as.numeric(loss$item())\n            \n            # Update params\n            self$update_weights(loss, learning_rate)\n          }\n        }\n        \n        cat(sprintf(\"Epoch %d/%d, Loss: %.4f\\n\", epoch, epochs, total_loss))\n      }\n    }\n    \n  )\n)"
  },
  {
    "objectID": "posts/2025-02-07-skip_gram_from_scratch/index.html#toy-example",
    "href": "posts/2025-02-07-skip_gram_from_scratch/index.html#toy-example",
    "title": "Word2Vec skip-gram from scratch in R",
    "section": "Toy example",
    "text": "Toy example\nLet’s craft 20 sentences corpus on which we’ll train our model (corpus generated by GPT o1).\n\ntext_corpus = \"My cat likes food. I give food to cat. Boy sees cat. Girl sees dog. Dog eats food. Cat sleeps now. Girl has cat. Boy feeds dog. Cat likes boy. Dog likes girl. Girl eats food. Boy runs home. Cat follows boy. Dog follows girl. Food is good. Cat is happy. Girl is happy. Boy is happy. Dog is hungry. They share food.\"\n\nWe apply some preprocessing steps:\n\nremove punctuation\nconvert text to lower-case\nsplit into tokens, where each token represent a word.\n\n\nprep_text = gsub(\"[[:punct:]]\", \"\", tolower(text_corpus))\ntokens = strsplit(prep_text, \"\\\\s+\") |&gt; unlist()\n\ntable(tokens) |&gt; as.data.frame()\n\n    tokens Freq\n1      boy    6\n2      cat    8\n3      dog    6\n4     eats    2\n5    feeds    1\n6  follows    2\n7     food    6\n8     girl    6\n9     give    1\n10    good    1\n11   happy    3\n12     has    1\n13    home    1\n14  hungry    1\n15       i    1\n16      is    5\n17   likes    3\n18      my    1\n19     now    1\n20    runs    1\n21    sees    2\n22   share    1\n23  sleeps    1\n24    they    1\n25      to    1\n\n\nIt gave us:\n\n\\(n  = 63\\) - number of tokens\n\\(m=25\\) - vocabulary size.\n\nNow we train our model with \\(d = 15\\), \\(l=1\\), \\(\\eta = 0.1\\) and \\(50\\) epochs.\n\n# set seed for reproducibility\nset.seed(1234)\ntorch_manual_seed(1234)\n\n# Create the model\nmodel = Word2Vec$new(tokens = tokens, embedding_dim = 15)\n\n# Run the model\nmodel$set_device()\n\nUsing GPU (CUDA)\n\nmodel$build_vocabulary()\nmodel$convert_tokens_to_indices()\nmodel$initialize_weights()\n\n# Train the model\nwindow_size  = 1   # each side\nepochs       = 50\nlearning_rate = 0.1\n\nmodel$train_model(window_size, epochs, learning_rate)\n\nEpoch 1/50, Loss: 746.5261\nEpoch 2/50, Loss: 447.8313\nEpoch 3/50, Loss: 383.7912\nEpoch 4/50, Loss: 356.2588\nEpoch 5/50, Loss: 341.8935\nEpoch 6/50, Loss: 333.6537\nEpoch 7/50, Loss: 327.7783\nEpoch 8/50, Loss: 323.0561\nEpoch 9/50, Loss: 319.0751\nEpoch 10/50, Loss: 315.6339\nEpoch 11/50, Loss: 312.6157\nEpoch 12/50, Loss: 309.9435\nEpoch 13/50, Loss: 307.5606\nEpoch 14/50, Loss: 305.4234\nEpoch 15/50, Loss: 303.4967\nEpoch 16/50, Loss: 301.7515\nEpoch 17/50, Loss: 300.1642\nEpoch 18/50, Loss: 298.7149\nEpoch 19/50, Loss: 297.3869\nEpoch 20/50, Loss: 296.1663\nEpoch 21/50, Loss: 295.0409\nEpoch 22/50, Loss: 294.0005\nEpoch 23/50, Loss: 293.0363\nEpoch 24/50, Loss: 292.1406\nEpoch 25/50, Loss: 291.3069\nEpoch 26/50, Loss: 290.5291\nEpoch 27/50, Loss: 289.8023\nEpoch 28/50, Loss: 289.1219\nEpoch 29/50, Loss: 288.4839\nEpoch 30/50, Loss: 287.8849\nEpoch 31/50, Loss: 287.3216\nEpoch 32/50, Loss: 286.7914\nEpoch 33/50, Loss: 286.2917\nEpoch 34/50, Loss: 285.8201\nEpoch 35/50, Loss: 285.3748\nEpoch 36/50, Loss: 284.9538\nEpoch 37/50, Loss: 284.5555\nEpoch 38/50, Loss: 284.1784\nEpoch 39/50, Loss: 283.8211\nEpoch 40/50, Loss: 283.4822\nEpoch 41/50, Loss: 283.1607\nEpoch 42/50, Loss: 282.8555\nEpoch 43/50, Loss: 282.5656\nEpoch 44/50, Loss: 282.2900\nEpoch 45/50, Loss: 282.0279\nEpoch 46/50, Loss: 281.7786\nEpoch 47/50, Loss: 281.5412\nEpoch 48/50, Loss: 281.3152\nEpoch 49/50, Loss: 281.0998\nEpoch 50/50, Loss: 280.8946\n\n\n\nPCA projection of embedding space\nOur final word embeddings are rows of the matrix \\(W_1\\) of dimensionality \\(25 \\times 15\\). We extract principal components of \\(W_1\\).\n\nembedding_space = model$W1 |&gt; as.matrix()\npca = prcomp(embedding_space, center = TRUE, scale. = TRUE)\nsummary(pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6     PC7\nStandard deviation     1.8047 1.5357 1.4169 1.18798 1.12567 1.01621 0.95311\nProportion of Variance 0.2171 0.1572 0.1338 0.09409 0.08448 0.06885 0.06056\nCumulative Proportion  0.2171 0.3743 0.5082 0.60228 0.68675 0.75560 0.81616\n                           PC8     PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     0.82368 0.72397 0.68359 0.65974 0.50026 0.46220 0.37462\nProportion of Variance 0.04523 0.03494 0.03115 0.02902 0.01668 0.01424 0.00936\nCumulative Proportion  0.86139 0.89633 0.92748 0.95650 0.97318 0.98743 0.99678\n                          PC15\nStandard deviation     0.21970\nProportion of Variance 0.00322\nCumulative Proportion  1.00000\n\n\nLet’s project embedding space into first two principal components which together explain \\(37.43\\%\\) of total variance.\n\nlibrary(ggplot2) # v3.5.1\n\npca_projection = embedding_space %*% pca$rotation[, 1:2]\nrownames(pca_projection) = names(model$vocabulary)\n\nggplot(data.frame(pca_projection), aes(x = PC1, y = PC2)) +\n  geom_point() +\n  geom_text(aes(label = rownames(pca_projection)), nudge_x = 0.1) +\n  theme_bw() +\n  labs(title = \"PCA Projection of Word Embeddings\")\n\n\n\n\n\n\n\n\nAs we can see, “similar” words are close each other:\n\nnouns boy and girl\nverbs sees, follows, runs, and likes\n\nHowever dog and cat are far apart. We might retrain the model with larger corpora, bigger embedding space, wider context window, more epochs, more robust optimizer (vanilla GD could be stuck in local minima). Altough words dog and sleeps, which are semantically related in case of my Labrador, are close each other."
  },
  {
    "objectID": "posts/2025-02-07-skip_gram_from_scratch/index.html#futher-steps",
    "href": "posts/2025-02-07-skip_gram_from_scratch/index.html#futher-steps",
    "title": "Word2Vec skip-gram from scratch in R",
    "section": "Futher steps",
    "text": "Futher steps\nIn practice, with huge vocabularies and enormous corpora, calculating denominator in the softmax function is quite expensive. To avoid that we use skip-gram with negative sampling aka SGNC, where having a pair (center word, context word) we pick randomly few context words as negative samples and fit binary logistic regression. For more details we refer to the chapter 6.8 of the book Speech and Language Processing by Daniel Jurafsky and James Martin."
  },
  {
    "objectID": "posts/2025-02-07-skip_gram_from_scratch/index.html#thinking-exercises",
    "href": "posts/2025-02-07-skip_gram_from_scratch/index.html#thinking-exercises",
    "title": "Word2Vec skip-gram from scratch in R",
    "section": "Thinking exercises",
    "text": "Thinking exercises\n\nIn fact, for each word \\(v_i\\) from the vocabulary \\(V\\) we have two word embeddings:\n\n\\(i\\)-th row of the matrix \\(W_1\\) where \\(v_i\\) served as a center word\n\\(i\\)-th column of the matrix \\(W_2\\) where \\(v_i\\) served as a context word.\n\nDo we get better word representation if combine (e.g. average) both vectors?\nSoftmax function has time complexity \\(\\mathcal{O}(n)\\). Replace it with hierarchical softmax which has time complexity \\(\\mathcal{O} (\\log n )\\)."
  },
  {
    "objectID": "posts/2025-02-07-skip_gram_from_scratch/index.html#references",
    "href": "posts/2025-02-07-skip_gram_from_scratch/index.html#references",
    "title": "Word2Vec skip-gram from scratch in R",
    "section": "References",
    "text": "References\n\nMikolov T. et al., Efficient Estimation of Word Representations in Vector Space, 2013, https://arxiv.org/abs/1301.3781\nBoykis V., What are embeddings?, https://vickiboykis.com/what_are_embeddings/\nChang W (2022). R6: Encapsulated Classes with Reference Semantics. https://r6.r-lib.org, https://github.com/r-lib/R6/.\nhttps://torch.mlverse.org/\nKundu S., Who’s Adam and What’s He Optimizing?, 2024, https://www.youtube.com/watch?v=MD2fYip6QsQ\nJurafsky D., Martin J. H., Speech and Language Processing, 2024, https://web.stanford.edu/~jurafsky/slp3/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "This is my blog where I share thoughts on different topics."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About Me",
    "section": "",
    "text": "This is my blog where I share thoughts on different topics."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pawel-wieczynski.github.io",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/2025-02-07-test_document/index.html",
    "href": "posts/2025-02-07-test_document/index.html",
    "title": "Test document",
    "section": "",
    "text": "This is example blog post to test if my Github Pages are configured correctly. Let’s plot formula of the function \\[\nf(x) = \\exp \\left( \\frac{1}{2}x \\right) + \\sin^2(x)\n\\]\n\n\n\n\nx = seq(-1, 1, 0.01)\ny = exp(x/2) + sin(x)^2\nplot(x, y, type = 'l')"
  },
  {
    "objectID": "posts/2025-02-07-test_document/index.html#test-document",
    "href": "posts/2025-02-07-test_document/index.html#test-document",
    "title": "Test document",
    "section": "",
    "text": "This is example blog post to test if my Github Pages are configured correctly. Let’s plot formula of the function \\[\nf(x) = \\exp \\left( \\frac{1}{2}x \\right) + \\sin^2(x)\n\\]\n\n\n\n\nx = seq(-1, 1, 0.01)\ny = exp(x/2) + sin(x)^2\nplot(x, y, type = 'l')"
  }
]